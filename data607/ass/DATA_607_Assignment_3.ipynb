{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA_607_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alby1976/Data607608Project/blob/master/data607/ass/DATA_607_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZKGd767pBrH"
      },
      "source": [
        "# DATA 607 -- Assignment 3\n",
        "\n",
        "Author: Albert Leung & Li Lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_9d6sv0pE1e"
      },
      "source": [
        "In this assignment, we apply the ideas underlying dense word embeddings like Word2Vec and GloVe to construct dense embeddings of categorical features.\n",
        "\n",
        "The context of our exploration will be the [Rossmann Store Sales Competition](https://www.kaggle.com/c/rossmann-store-sales/overview/description) from *Kaggle*, the goal of which is to forecast store sales using store, promotion, and competitor data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1ckGfSwPtz"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "1. Download the data from the competition page or from [my github](https://github.com/mgreenbe/rossmann).\n",
        "\n",
        "2. Replace each date in the `Date` column with number of days between it and January 1, 2013, the earliest date in the table.\n",
        "\n",
        "3. Use `pd.get_dummies` to construct dataframes `stores`, `days_of_week`, and `state_holidays` containing 1-hot encodings of the categorical variables `Store`, `DayOfWeek`, and `StateHoliday`, respectively.\n",
        "\n",
        "4. Assemble these encoded features, together with the numerical ones (`Date`, `Customers`) and binary ones (`Open`, `Promo`, `SchoolHoliday`), in a matrix `X`, the first 1115 columns of which represent the store ID.\n",
        "\n",
        "5. Split the data `X` and `Y` into training and validation sets. Standardize the numerical feature columns. Here, the relevant means and standard deviations should be computed from *training data*.\n",
        "\n",
        "6. Train the model `MyModel`, below, using `MeanSquaredLogarithmicError` as the loss function. Explain, briefly, why this is an appropriate choice of loss function. Stop training when validation error stabilizes.\n",
        "\n",
        "7. **(Optional)** Add hidden layers to this model and tune the `store_emb_dim` hyperparameter to improve your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsAAhZqjFArh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Lsjk8Yq_4Y"
      },
      "source": [
        "!pip install nbconvert\n",
        "!apt-get update\n",
        "!apt-get install pandoc \n",
        "!add-apt-repository universe\n",
        "!add-apt-repository ppa:inkscape.dev/stable\n",
        "!apt-get update\n",
        "!apt install inkscape\n",
        "!apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended\n",
        "#!jupyter nbconvert DATA_607__Assignment_1.ipynb --to pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHF6Hjr4rmmC"
      },
      "source": [
        "#mount Google drive\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "ROOT = '/content/drive' # default for the drive\n",
        "print(ROOT)\n",
        "drive.mount(ROOT) # we mount the drive at /content/drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyGLq_aaE_mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdec44ba-3cac-4f9a-bc86-24d88ca3deba"
      },
      "source": [
        "!pip install --user h5py==2.10.0\n",
        "!pip install --upgrade scikit-learn keras tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeB4QrdWP7nz",
        "outputId": "997bdf42-1fe3-47ca-fc3a-df74ed394a4b"
      },
      "source": [
        "#convert csv to hdf5s\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "base_uri = 'https://raw.githubusercontent.com/mgreenbe/rossmann/main/'\n",
        "df = pd.read_csv(base_uri + 'train.csv', parse_dates=['Date'], infer_datetime_format=True, \n",
        "                 dtype={'Store' : 'category', 'DayOfWeek' : 'category', 'StateHoliday' : 'category', \n",
        "                        'Open' : np.int8, 'Promo' : np.int8, 'SchoolHoliday' : np.int8})\n",
        "df.Date = (df.Date - datetime.datetime.strptime('2013-01-01','%Y-%m-%d')).dt.days\n",
        "df.to_hdf('train.hdf5', key='train', complevel=9, mode='w', format='table', data_columns=True)\n",
        "\n",
        "#generate stores dataframe \n",
        "stores = pd.get_dummies(df.Store, prefix='Store')\n",
        "stores.to_hdf('stores.hdf5', key='stores', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "#generate days_of_week dataframe\n",
        "days_of_week = pd.get_dummies(df.DayOfWeek, prefix='Day_Of_Week')\n",
        "days_of_week.to_hdf('days_of_week.hdf5', key='days_of_week', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "#genearte state_holidays dataframe\n",
        "state_holidays = pd.get_dummies(df.StateHoliday, prefix='State_Hoiday')\n",
        "state_holidays.to_hdf('state_holidays.hdf5', key='days_of_week', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "#merge dummy tables together\n",
        "categorical = stores.merge(days_of_week, left_index=True, right_index=True, how='inner', copy=False)\n",
        "categorical = categorical.merge(state_holidays, left_index=True, right_index=True, how='inner', copy=False)\n",
        "categorical.to_hdf('categorical.hdf5', key='categorical', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "del stores, days_of_week, state_holidays\n",
        "\n",
        "#\n",
        "sales = df.Sales\n",
        "sales.to_hdf('sales.hdf5', key='sales', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "df.drop(columns=['Store', 'DayOfWeek', 'Sales', 'StateHoliday'], inplace=True)\n",
        "\n",
        "dataset = categorical.merge(df, left_index=True, right_index=True, copy=False)\n",
        "dataset.to_hdf('dataset.hdf5', key='dataset', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "del df\n",
        "gc.collect()\n",
        "dataset.info()\n",
        "\n",
        "\n",
        "#Split the orignal data into the corresponding traing and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, sales, test_size=0.20, random_state=42)\n",
        "\n",
        "X_train.to_hdf('X_train.hdf5', key='x_train', complevel=9, mode='w', data_columns=True)\n",
        "y_train.to_hdf('y_train.hdf5', key='y_train', complevel=9, mode='w', data_columns=True)\n",
        "X_test.to_hdf('X_test.hdf5', key='x_test', complevel=9, mode='w', data_columns=True)\n",
        "y_test.to_hdf('y_test.hdf5', key='y_test', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "del dataset, sales, X_train, X_test, y_train, y_test\n",
        "gc.collect()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1017209 entries, 0 to 1017208\n",
            "Columns: 1131 entries, Store_1 to SchoolHoliday\n",
            "dtypes: int64(2), int8(3), uint8(1126)\n",
            "memory usage: 1.1 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd9M0XgK0Erz"
      },
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self, n_stores=1115, store_emb_dim=4):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.n_stores = n_stores\n",
        "    self.encoder = keras.layers.Dense(store_emb_dim, name=\"encoder\")\n",
        "    self.hidden_1 = tf.keras.layers.Dense(store_emb_dim, name=\"hidden_1\", activation=\"exponential\")\n",
        "    self.drop_1 = tf.keras.layers.Dropout(0.3, name='drop_1')\n",
        "    self.hidden_2 = tf.keras.layers.Dense(store_emb_dim, name=\"hidden_2\", activation=\"relu\")\n",
        "    self.drop_2 = tf.keras.layers.Dropout(0.2, name='drop_2')\n",
        "    self.regressor = keras.layers.Dense(1, name=\"regressor\")\n",
        "\n",
        "  def call(self, X):\n",
        "    x = tf.concat([self.encoder(X[:, :self.n_stores]), X[:, self.n_stores:]], axis=-1)\n",
        "    x = self.hidden_1(x)\n",
        "    x = self.drop_1(x)\n",
        "    x = self.hidden_2(x)\n",
        "    x = self.drop_2(x)\n",
        "    return self.regressor(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBDM74hBK7k",
        "outputId": "008322fa-eb6e-47e5-8e3c-da0f7a3635eb"
      },
      "source": [
        "#from tensorflow.keras.layers.experimental import preprocessing\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# def normalize(data, input):\n",
        "#   layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "#   layer.adapt(data)\n",
        "\n",
        "#   return layer(input)\n",
        "\n",
        "#reading training set\n",
        "X_train = pd.read_hdf('X_train.hdf5', key='x_train')\n",
        "normalize = preprocessing.StandardScaler()\n",
        "#X_train[['Date','Customers']] = normalize(X_train[['Date', 'Customers']].to_numpy(), X_train[['Date', 'Customers']].to_numpy())\n",
        "X_train[['Date','Customers']] = normalize.fit_transform(X_train[[\"Date\",\"Customers\"]])\n",
        "y_train = pd.read_hdf('y_train.hdf5', key='y_train')\n",
        "\n",
        "#reading testing set\n",
        "X_test = pd.read_hdf('X_test.hdf5', key='x_test')\n",
        "#X_test[['Date','Customers']] = normalize((X_train[['Date', 'Customers']]).to_numpy(), (X_test[['Date', 'Customers']]).to_numpy())\n",
        "X_test[['Date','Customers']] = normalize.transform(X_test[[\"Date\",\"Customers\"]])\n",
        "\n",
        "y_test = pd.read_hdf('y_test.hdf5', key='y_test')\n",
        "\n",
        "#print(type(X_train),'\\n',X_train.to_numpy())\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9MZ3wYi06a2"
      },
      "source": [
        "- This is model is *not* built with `keras.models.Sequential` -- it's not simply passing data through a sequence of layers. The first 1115 columns of the input, representing the store ID, are projected onto a `store_emb_dim`-dimensional space. The resulting projections are then concatenated with the remaining features before applying linear regression. (Notice the absence of nonlinear activation functions.)\n",
        "\n",
        "- **Warning:** The data set contains > 1 million rows. To avoid running out of memory, work initially with a subset of the rows (say, 10,000). Train on as large a subset of the whole dataset as you can without crashing your session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4HM_In53CRe"
      },
      "source": [
        "## References\n",
        "\n",
        "Rachel Thomas, [An Introduction to Deep Learning for Tabular Data](https://www.fast.ai/2018/04/29/categorical-embeddings/) (fast.ai blog, April 29, 2018)\n",
        "\n",
        "Cheng Guo and Felix Berkhahn, [Entity Embeddings of Categorical Variables](https://arxiv.org/pdf/1604.06737.pdf) (April 25, 2016)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU0w1m1-54AG"
      },
      "source": [
        "import gc\n",
        "\n",
        "#del model\n",
        "gc.collect()\n",
        "model = MyModel()\n",
        "loss = tf.keras.losses.MeanSquaredLogarithmicError()\n",
        "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=10), metrics=[\"accuracy\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "615uWQ3zpN_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccab102-9b92-44ca-83a3-7de53ec67edc"
      },
      "source": [
        "gc.collect()\n",
        "model.fit(X_train, y_train.to_numpy(), validation_data=(X_test, y_test.to_numpy()) , epochs=50)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "25431/25431 [==============================] - 80s 3ms/step - loss: 63.8164 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 2/50\n",
            "25431/25431 [==============================] - 77s 3ms/step - loss: 63.7985 - accuracy: 0.1701 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 3/50\n",
            "25431/25431 [==============================] - 77s 3ms/step - loss: 63.7924 - accuracy: 0.1703 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 4/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8499 - accuracy: 0.1695 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 5/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8520 - accuracy: 0.1695 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 6/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.7690 - accuracy: 0.1705 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 7/50\n",
            "25431/25431 [==============================] - 77s 3ms/step - loss: 63.7935 - accuracy: 0.1702 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 8/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.7476 - accuracy: 0.1708 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 9/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8427 - accuracy: 0.1696 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 10/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8390 - accuracy: 0.1697 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 11/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.7868 - accuracy: 0.1701 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 12/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8259 - accuracy: 0.1698 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 13/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.7974 - accuracy: 0.1702 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 14/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.8033 - accuracy: 0.1700 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 15/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8078 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 16/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8536 - accuracy: 0.1693 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 17/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.7993 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 18/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.7883 - accuracy: 0.1703 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 19/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.7837 - accuracy: 0.1703 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 20/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8858 - accuracy: 0.1690 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 21/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.7993 - accuracy: 0.1701 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 22/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.8054 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 23/50\n",
            "25431/25431 [==============================] - 76s 3ms/step - loss: 63.8311 - accuracy: 0.1697 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 24/50\n",
            "25431/25431 [==============================] - 75s 3ms/step - loss: 63.8218 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 25/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.7802 - accuracy: 0.1703 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 26/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8439 - accuracy: 0.1696 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 27/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.7752 - accuracy: 0.1704 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 28/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8223 - accuracy: 0.1697 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 29/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8081 - accuracy: 0.1700 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 30/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8304 - accuracy: 0.1700 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 31/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8000 - accuracy: 0.1701 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 32/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8097 - accuracy: 0.1698 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 33/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8501 - accuracy: 0.1695 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 34/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8192 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 35/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8468 - accuracy: 0.1697 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 36/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8620 - accuracy: 0.1694 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 37/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.8552 - accuracy: 0.1695 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 38/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8066 - accuracy: 0.1699 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 39/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8263 - accuracy: 0.1697 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 40/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8368 - accuracy: 0.1696 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 41/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8031 - accuracy: 0.1700 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 42/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.7929 - accuracy: 0.1704 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 43/50\n",
            "25431/25431 [==============================] - 72s 3ms/step - loss: 63.8153 - accuracy: 0.1698 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 44/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.7906 - accuracy: 0.1702 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 45/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8008 - accuracy: 0.1701 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 46/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.7609 - accuracy: 0.1706 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 47/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.8082 - accuracy: 0.1700 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 48/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.7998 - accuracy: 0.1700 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 49/50\n",
            "25431/25431 [==============================] - 74s 3ms/step - loss: 63.7878 - accuracy: 0.1701 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Epoch 50/50\n",
            "25431/25431 [==============================] - 73s 3ms/step - loss: 63.7823 - accuracy: 0.1703 - val_loss: 63.7871 - val_accuracy: 0.1704\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder (Dense)              multiple                  4464      \n",
            "_________________________________________________________________\n",
            "hidden_1 (Dense)             multiple                  84        \n",
            "_________________________________________________________________\n",
            "drop_1 (Dropout)             multiple                  0         \n",
            "_________________________________________________________________\n",
            "hidden_2 (Dense)             multiple                  20        \n",
            "_________________________________________________________________\n",
            "drop_2 (Dropout)             multiple                  0         \n",
            "_________________________________________________________________\n",
            "regressor (Dense)            multiple                  5         \n",
            "=================================================================\n",
            "Total params: 4,573\n",
            "Trainable params: 4,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01RvKLjip-Gp"
      },
      "source": [
        "Mean squared logarithmic error is the measure of the ratio between true and predicted values. It is suitable for a loss function in regression because the target values can differ in orders of magnitudes and we do not want large errors to be penalized more than small errors. This metric is best to use when targets have expoential growth. "
      ]
    }
  ]
}