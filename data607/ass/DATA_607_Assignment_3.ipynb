{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA_607_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alby1976/Data607608Project/blob/master/data607/ass/DATA_607_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZKGd767pBrH"
      },
      "source": [
        "# DATA 607 -- Assignment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_9d6sv0pE1e"
      },
      "source": [
        "In this assignment, we apply the ideas underlying dense word embeddings like Word2Vec and GloVe to construct dense embeddings of categorical features.\n",
        "\n",
        "The context of our exploration will be the [Rossmann Store Sales Competition](https://www.kaggle.com/c/rossmann-store-sales/overview/description) from *Kaggle*, the goal of which is to forecast store sales using store, promotion, and competitor data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1ckGfSwPtz"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "1. Download the data from the competition page or from [my github](https://github.com/mgreenbe/rossmann).\n",
        "\n",
        "2. Replace each date in the `Date` column with number of days between it and January 1, 2013, the earliest date in the table.\n",
        "\n",
        "3. Use `pd.get_dummies` to construct dataframes `stores`, `days_of_week`, and `state_holidays` containing 1-hot encodings of the categorical variables `Store`, `DayOfWeek`, and `StateHoliday`, respectively.\n",
        "\n",
        "4. Assemble these encoded features, together with the numerical ones (`Date`, `Customers`) and binary ones (`Open`, `Promo`, `SchoolHoliday`), in a matrix `X`, the first 1115 columns of which represent the store ID.\n",
        "\n",
        "5. Split the data `X` and `Y` into training and validation sets. Standardize the numerical feature columns. Here, the relevant means and standard deviations should be computed from *training data*.\n",
        "\n",
        "6. Train the model `MyModel`, below, using `MeanSquaredLogarithmicError` as the loss function. Explain, briefly, why this is an appropriate choice of loss function. Stop training when validation error stabilizes.\n",
        "\n",
        "7. **(Optional)** Add hidden layers to this model and tune the `store_emb_dim` hyperparameter to improve your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd9M0XgK0Erz"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self, n_stores=1115, store_emb_dim=20):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.n_stores = n_stores\n",
        "    self.encoder = keras.layers.Dense(store_emb_dim, name=\"encoder\")\n",
        "    self.regressor = keras.layers.Dense(1, name=\"regressor\")\n",
        "\n",
        "  def call(self, X):\n",
        "    x = tf.concat([self.encoder(X[:, :self.n_stores]), X[:, self.n_stores:]], axis=-1)\n",
        "    return self.regressor(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9MZ3wYi06a2"
      },
      "source": [
        "- This is model is *not* built with `keras.models.Sequential` -- it's not simply passing data through a sequence of layers. The first 1115 columns of the input, representing the store ID, are projected onto a `store_emb_dim`-dimensional space. The resulting projections are then concatenated with the remaining features before applying linear regression. (Notice the absence of nonlinear activation functions.)\n",
        "\n",
        "- **Warning:** The data set contains > 1 million rows. To avoid running out of memory, work initially with a subset of the rows (say, 10,000). Train on as large a subset of the whole dataset as you can without crashing your session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4HM_In53CRe"
      },
      "source": [
        "## References\n",
        "\n",
        "Rachel Thomas, [An Introduction to Deep Learning for Tabular Data](https://www.fast.ai/2018/04/29/categorical-embeddings/) (fast.ai blog, April 29, 2018)\n",
        "\n",
        "Cheng Guo and Felix Berkhahn, [Entity Embeddings of Categorical Variables](https://arxiv.org/pdf/1604.06737.pdf) (April 25, 2016)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU0w1m1-54AG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}