{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA_607_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alby1976/Data607608Project/blob/master/data607/ass/DATA_607_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZKGd767pBrH"
      },
      "source": [
        "# DATA 607 -- Assignment 3\n",
        "\n",
        "Author: Albert Leung & Li Lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_9d6sv0pE1e"
      },
      "source": [
        "In this assignment, we apply the ideas underlying dense word embeddings like Word2Vec and GloVe to construct dense embeddings of categorical features.\n",
        "\n",
        "The context of our exploration will be the [Rossmann Store Sales Competition](https://www.kaggle.com/c/rossmann-store-sales/overview/description) from *Kaggle*, the goal of which is to forecast store sales using store, promotion, and competitor data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1ckGfSwPtz"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "1. Download the data from the competition page or from [my github](https://github.com/mgreenbe/rossmann).\n",
        "\n",
        "2. Replace each date in the `Date` column with number of days between it and January 1, 2013, the earliest date in the table.\n",
        "\n",
        "3. Use `pd.get_dummies` to construct dataframes `stores`, `days_of_week`, and `state_holidays` containing 1-hot encodings of the categorical variables `Store`, `DayOfWeek`, and `StateHoliday`, respectively.\n",
        "\n",
        "4. Assemble these encoded features, together with the numerical ones (`Date`, `Customers`) and binary ones (`Open`, `Promo`, `SchoolHoliday`), in a matrix `X`, the first 1115 columns of which represent the store ID.\n",
        "\n",
        "5. Split the data `X` and `Y` into training and validation sets. Standardize the numerical feature columns. Here, the relevant means and standard deviations should be computed from *training data*.\n",
        "\n",
        "6. Train the model `MyModel`, below, using `MeanSquaredLogarithmicError` as the loss function. Explain, briefly, why this is an appropriate choice of loss function. Stop training when validation error stabilizes.\n",
        "\n",
        "7. **(Optional)** Add hidden layers to this model and tune the `store_emb_dim` hyperparameter to improve your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsAAhZqjFArh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cwv3G5tFAJn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyGLq_aaE_mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aec9652-6e33-4822-d964-0cf2c312fcc7"
      },
      "source": [
        "!pip install --user h5py==2.10.0\n",
        "!pip install --upgrade scikit-learn keras tensorflow"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.24.1)\n",
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeB4QrdWP7nz",
        "outputId": "282117a4-e735-42d0-f477-5ccd89339165"
      },
      "source": [
        "#convert csv to hdf5s\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "base_uri = 'https://raw.githubusercontent.com/mgreenbe/rossmann/main/'\n",
        "df = pd.read_csv(base_uri + 'train.csv', parse_dates=['Date'], infer_datetime_format=True, \n",
        "                 dtype={'Store' : 'category', 'DayOfWeekStore' : 'category', 'DayOfWeek' : 'category', 'StateHoliday' : 'category', \n",
        "                        'Open' : np.int8, 'Promo' : np.int8, 'SchoolHoliday' : np.int8})\n",
        "df.Date = (df.Date - datetime.datetime.strptime('2013-01-01','%Y-%m-%d')).dt.days\n",
        "df.to_hdf('train.hdf5', key='train', complevel=9, mode='w', format='table', data_columns=True)\n",
        "\n",
        "#generate stores dataframe \n",
        "stores = pd.get_dummies(df.Store, prefix='Store')\n",
        "stores.to_hdf('stores.hdf5', key='stores', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "#generate days_of_week dataframe\n",
        "days_of_week = pd.get_dummies(df.DayOfWeek, prefix='Day_Of_Week')\n",
        "days_of_week.to_hdf('days_of_week.hdf5', key='days_of_week', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "#genearte state_holidays dataframe\n",
        "state_holidays = pd.get_dummies(df.StateHoliday, prefix='State_Hoiday')\n",
        "state_holidays.to_hdf('state_holidays.hdf5', key='days_of_week', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "#merge dummy tables together\n",
        "categorical = stores.merge(days_of_week, left_index=True, right_index=True, how='inner', copy=False)\n",
        "categorical = categorical.merge(state_holidays, left_index=True, right_index=True, how='inner', copy=False)\n",
        "categorical.to_hdf('categorical.hdf5', key='categorical', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "del stores, days_of_week, state_holidays\n",
        "\n",
        "#\n",
        "sales = df.Sales\n",
        "sales.to_hdf('sales.hdf5', key='sales', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "df.drop(columns=['Store', 'DayOfWeek', 'Sales', 'StateHoliday'], inplace=True)\n",
        "\n",
        "dataset = categorical.merge(df, left_index=True, right_index=True, copy=False)\n",
        "dataset.to_hdf('dataset.hdf5', key='dataset', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "del df\n",
        "gc.collect()\n",
        "dataset.info()\n",
        "\n",
        "\n",
        "#Split the orignal data into the corresponding traing and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, sales, test_size=0.20, random_state=42)\n",
        "\n",
        "X_train.to_hdf('X_train.hdf5', key='x_train', complevel=9, mode='w', data_columns=True)\n",
        "y_train.to_hdf('y_train.hdf5', key='y_train', complevel=9, mode='w', data_columns=True)\n",
        "X_test.to_hdf('X_test.hdf5', key='x_test', complevel=9, mode='w', data_columns=True)\n",
        "y_test.to_hdf('y_test.hdf5', key='y_test', complevel=9, mode='w', data_columns=True)\n",
        "\n",
        "del dataset, sales, X_train, X_test, y_train, y_test\n",
        "gc.collect()\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1017209 entries, 0 to 1017208\n",
            "Columns: 1131 entries, Store_1 to SchoolHoliday\n",
            "dtypes: int64(2), int8(3), uint8(1126)\n",
            "memory usage: 1.1 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd9M0XgK0Erz"
      },
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self, n_stores=1115, store_emb_dim=20):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.n_stores = n_stores\n",
        "    self.encoder = keras.layers.Dense(store_emb_dim, name=\"encoder\")\n",
        "    self.regressor = keras.layers.Dense(1, name=\"regressor\")\n",
        "\n",
        "  def call(self, X):\n",
        "    x = tf.concat([self.encoder(X[:, :self.n_stores]), X[:, self.n_stores:]], axis=-1)\n",
        "    return self.regressor(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9MZ3wYi06a2"
      },
      "source": [
        "- This is model is *not* built with `keras.models.Sequential` -- it's not simply passing data through a sequence of layers. The first 1115 columns of the input, representing the store ID, are projected onto a `store_emb_dim`-dimensional space. The resulting projections are then concatenated with the remaining features before applying linear regression. (Notice the absence of nonlinear activation functions.)\n",
        "\n",
        "- **Warning:** The data set contains > 1 million rows. To avoid running out of memory, work initially with a subset of the rows (say, 10,000). Train on as large a subset of the whole dataset as you can without crashing your session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4HM_In53CRe"
      },
      "source": [
        "## References\n",
        "\n",
        "Rachel Thomas, [An Introduction to Deep Learning for Tabular Data](https://www.fast.ai/2018/04/29/categorical-embeddings/) (fast.ai blog, April 29, 2018)\n",
        "\n",
        "Cheng Guo and Felix Berkhahn, [Entity Embeddings of Categorical Variables](https://arxiv.org/pdf/1604.06737.pdf) (April 25, 2016)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU0w1m1-54AG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}